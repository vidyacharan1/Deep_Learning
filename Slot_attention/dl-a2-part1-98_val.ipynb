{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-05T12:44:44.974559Z","iopub.status.busy":"2024-05-05T12:44:44.974281Z","iopub.status.idle":"2024-05-05T12:44:48.454570Z","shell.execute_reply":"2024-05-05T12:44:48.453668Z","shell.execute_reply.started":"2024-05-05T12:44:44.974534Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","class SlotAttentionModule(nn.Module):\n","    def __init__(self, num_slots, num_iters, slot_size, mlp_hidden_dim, eps=1e-8):\n","        super(SlotAttentionModule, self).__init__()\n","        self.num_slots = num_slots\n","        self.num_iters = num_iters\n","        self.slot_size = slot_size\n","        self.eps = eps\n","        self.mlp_hidden_dim = mlp_hidden_dim\n","\n","        self.input_norm = nn.LayerNorm(slot_size)\n","        self.mu_slot = nn.Parameter(torch.randn(1, 1, slot_size))\n","        self.sigma_slot = nn.Parameter(torch.randn(1, 1, slot_size))\n","        self.k_proj = nn.Linear(slot_size, slot_size, bias=False)\n","        self.q_proj = nn.Linear(slot_size, slot_size, bias=False)\n","        self.v_proj = nn.Linear(slot_size, slot_size, bias=False)\n","        self.slots_norm = nn.LayerNorm(slot_size)\n","        self.gru = nn.GRUCell(slot_size, slot_size)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(slot_size, mlp_hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(mlp_hidden_dim, slot_size)\n","        )\n","        self.mlp_norm = nn.LayerNorm(slot_size)\n","\n","    def forward(self, x):\n","        inputs = self.input_norm(x)\n","        k = self.k_proj(inputs)\n","        v = self.v_proj(inputs)\n","        slots = self.mu_slot + torch.exp(self.sigma_slot) * torch.randn((x.size(0), self.num_slots, self.slot_size), device=x.device)\n","\n","        for _ in range(self.num_iters):\n","            slots_prev = slots\n","            slots = self.slots_norm(slots)\n","            q = self.q_proj(slots)\n","            q = q * torch.sqrt(torch.tensor(1/self.slot_size, dtype=torch.float32, device=x.device))\n","            attn = F.softmax(torch.bmm(k, q.transpose(1, 2)), dim=-1)\n","            attn = (attn + self.eps) / torch.sum(attn, dim=-2, keepdim=True)\n","            # print(attn.shape, v.shape)\n","            attn_sh = attn.permute(0, 2, 1)\n","            updates = torch.bmm(attn_sh, v)\n","            # print(updates.shape, slots_prev.shape)\n","            upd = updates.reshape(-1,self.slot_size)\n","            sl_pr = slots_prev.reshape(-1,self.slot_size)\n","            slots = self.gru(upd, sl_pr)\n","            # print(slots.shape, slots_prev.shape)\n","            slots_new = slots.reshape(-1, self.num_slots, self.slot_size)\n","            slots = slots_new + self.mlp(self.mlp_norm(slots_new))\n","        return slots\n","    \n","# samodule = SlotAttentionModule(11, 3, 64, 128)\n","# x = torch.randn(64,128*128,64)\n","# y = samodule(x)\n","# print(y.shape)\n","\n","\n","def grid_embed(resolution):\n","    scope = [np.linspace(0.0, 1.0, num=x) for x in resolution]\n","    grid = np.meshgrid(*scope, indexing='ij', sparse=False)\n","    grid = np.stack(grid, axis=-1)\n","    grid = np.reshape(grid, (resolution[0], resolution[1], -1))\n","    grid = np.expand_dims(grid, axis=0)\n","    grid = grid.astype(np.float32)\n","    # print(grid.shape)\n","    return torch.cat([torch.tensor(grid), 1.0 - torch.tensor(grid)], dim=-1)\n","\n","# print(grid_embed((128, 128)).shape)\n","\n","class SlotAttentionNetwork(nn.Module):\n","    def __init__(self, num_slots, num_iters, resolution):\n","        super(SlotAttentionNetwork, self).__init__()\n","        self.num_slots = num_slots\n","        self.num_iters = num_iters\n","        self.resolution = resolution\n","\n","        self.cnn_encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 5, padding=2),\n","            nn.ReLU()\n","        )\n","        self.norm = nn.LayerNorm(64)\n","        self.dense_encode = nn.Linear(4, 64)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64)\n","        )\n","        self.slot_attention = SlotAttentionModule(num_slots, num_iters, 64, 128)\n","        self.decoder_size = (8, 8)\n","        self.dense_decode = nn.Linear(4, 64)\n","        self.cnn_decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 64, 5, stride=1, padding=2),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 4, 3, stride=1, padding=1)\n","        )\n","\n","    def forward(self, input):\n","        x = self.cnn_encoder(input)\n","        x = x.permute(0, 2, 3, 1)\n","        temp = grid_embed(self.resolution).to(device)\n","        # temp = temp.view(1, -1, temp.size(-1))\n","        x = x + self.dense_encode(temp)\n","        x = x.view(-1, x.size(1) * x.size(2), x.size(-1))\n","        x = self.mlp(self.norm(x))\n","        slots = self.slot_attention(x)\n","        x = slots.view(-1, slots.shape[2])[:, None, None, :]\n","        x = x.repeat(1, self.decoder_size[0], self.decoder_size[1], 1)\n","        x = x + self.dense_decode(grid_embed(self.decoder_size).to(device))\n","        x = x.permute(0, 3, 1, 2)\n","        x = self.cnn_decoder(x)\n","        x = x.view(input.shape[0],11,4,128,128)\n","        recons, masks = torch.split(x, (3, 1), dim=2)\n","        masks = torch.sigmoid(masks)\n","        combined = torch.sum(recons * masks, dim=1)\n","        return recons, masks, combined, slots\n","    \n","# model = SlotAttentionNetwork(11, 3, (128, 128))\n","# x = torch.randn(32, 3, 128, 128)\n","# recons, masks, combined, slots = model(x)\n","# print(recons.shape, masks.shape, combined.shape, slots.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T12:44:48.457174Z","iopub.status.busy":"2024-05-05T12:44:48.456369Z","iopub.status.idle":"2024-05-05T12:44:50.938613Z","shell.execute_reply":"2024-05-05T12:44:50.937839Z","shell.execute_reply.started":"2024-05-05T12:44:48.457139Z"},"trusted":true},"outputs":[],"source":["# hyperparameters\n","batch_size = 64\n","num_slots = 11\n","num_iters = 3\n","resolution = (128, 128)\n","\n","model = SlotAttentionNetwork(num_slots, num_iters, resolution)\n","\n","model = nn.DataParallel(model)\n","\n","# The dataset is in the folder '/kaggle/input/clevrtex-images/train'\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.98 ** epoch)\n","criterion = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T12:44:50.940279Z","iopub.status.busy":"2024-05-05T12:44:50.939817Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import os\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.images = os.listdir(root_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.root_dir, self.images[idx])\n","        image = Image.open(img_name).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","# Define transform\n","transform = transforms.Compose([transforms.CenterCrop((128, 128)), transforms.ToTensor()])\n","\n","# Load dataset\n","data_dir = '/kaggle/input/clevrtex-images/train/train'  \n","dataset = CustomDataset(data_dir, transform=transform)\n","# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","# make train and validation split\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Train the model\n","num_epochs = 35\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_losses = []\n","val_losses = []\n","# Model training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    # for images in dataloader:\n","    for images in train_loader:\n","        images = images.to(device)\n","#         print(images.shape)\n","        recons, masks, combined, slots = model(images)\n","#         print(combined.shape, images.shape)\n","        loss = criterion(combined, images)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    scheduler.step()\n","    train_losses.append(loss.item())\n","    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n","\n","    model.eval()\n","    val_loss = 0\n","    for images in val_loader:\n","        images = images.to(device)\n","        recons, masks, combined, slots = model(images)\n","        loss = criterion(combined, images)\n","    val_losses.append(loss.item())\n","    print(f'Epoch: {epoch+1}, Validation Loss: {loss.item()}')\n","    \n","    if (epoch % 3) == 0:\n","        torch.save(model.state_dict(), f'model_checkpoint_98_{epoch}.pt')\n","\n","torch.save(model.state_dict(), 'model_A2_98.pt')\n","\n","# Plot the training and validation losses\n","import matplotlib.pyplot as plt\n","\n","# Plot the training and validation losses vs epochs\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4797203,"sourceId":8125472,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
